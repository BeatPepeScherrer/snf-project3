{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e73f9ebb",
      "metadata": {},
      "source": [
        "# Unsupervised Theme Discovery for Supply-Chain Sustainability Texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa5924a6",
      "metadata": {},
      "source": [
        "This notebook helps you discover data-driven themes in your corpus (e.g., 700 story + response pairs).\n",
        "It follows a **map → cluster → label** pattern using text embeddings and clustering, then surfaces\n",
        "representative quotes and exports results for further analysis.\n",
        "\n",
        "**What you'll get**\n",
        "- Cleaned corpus (merging story and response text per case)\n",
        "- Vector embeddings (Sentence-Transformers or OpenAI embeddings)\n",
        "- Dimensionality reduction (UMAP) for visualization\n",
        "- Clustering (HDBSCAN by default; k-means as a fallback)\n",
        "- Automatic keyword-based labels per cluster (c-TF-IDF)\n",
        "- Representative quotes/examples per cluster\n",
        "- CSV exports you can use downstream\n",
        "\n",
        "> Tip: Treat this as *broad discovery*; you'll later do schema-based extraction for your RQs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23f22fad",
      "metadata": {},
      "source": [
        "## 0) Prerequisites\n",
        "\n",
        "Install these packages in your environment (e.g., with `pip install ...`):\n",
        "\n",
        "```\n",
        "pip install pandas numpy scikit-learn umap-learn hdbscan sentence-transformers nltk tqdm\n",
        "# Optional (for OpenAI-based embeddings or labels)\n",
        "pip install openai\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf4d5c51",
      "metadata": {},
      "source": [
        "## 1) Load your data\n",
        "\n",
        "Expected input: a CSV or JSONL with at least these columns:\n",
        "- `Companies`, `Company Sectors`, `Company Headquarters`, `Countries`, `Backdate`\n",
        "- `story_text` and `response_text`\n",
        "\n",
        "You can also adapt the loader if your schema differs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fd3fbc18",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded rows: 243\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Companies</th>\n",
              "      <th>Company Sectors</th>\n",
              "      <th>Company Headquarters</th>\n",
              "      <th>Countries</th>\n",
              "      <th>Regions</th>\n",
              "      <th>Response Sectors</th>\n",
              "      <th>Backdate</th>\n",
              "      <th>Title</th>\n",
              "      <th>Responded To</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Responded</th>\n",
              "      <th>Authors</th>\n",
              "      <th>URL</th>\n",
              "      <th>Link to Company Page</th>\n",
              "      <th>Story</th>\n",
              "      <th>Response</th>\n",
              "      <th>story_text</th>\n",
              "      <th>response_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Shell plc</td>\n",
              "      <td>Hydrogen|Oil, gas &amp; coal</td>\n",
              "      <td>GB</td>\n",
              "      <td>IE</td>\n",
              "      <td></td>\n",
              "      <td>Oil, gas &amp; coal</td>\n",
              "      <td>02.06.2009</td>\n",
              "      <td>Shell response re Corrib gas protest</td>\n",
              "      <td>Ireland: Archbishop Desmond Tutu raises concer...</td>\n",
              "      <td>Protests|Violence</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Terry Nolan, Managing Director, Shell E&amp;P Ireland</td>\n",
              "      <td>https://www.business-humanrights.org/en/latest...</td>\n",
              "      <td>https://www.business-humanrights.org/en/compan...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Ireland: Archbishop Desmond Tutu raises concer...</td>\n",
              "      <td>We wish to state categorically that there was ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Shell plc</td>\n",
              "      <td>Hydrogen|Oil, gas &amp; coal</td>\n",
              "      <td>GB</td>\n",
              "      <td>NG</td>\n",
              "      <td></td>\n",
              "      <td>Oil, gas &amp; coal</td>\n",
              "      <td>08.07.2009</td>\n",
              "      <td>Shell response re climate change report</td>\n",
              "      <td>NGO report on Shell's impact on climate change...</td>\n",
              "      <td>Clean, Healthy &amp; Sustainable Environment|Clima...</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Shell</td>\n",
              "      <td>https://www.business-humanrights.org/en/latest...</td>\n",
              "      <td>https://www.business-humanrights.org/en/compan...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>NGO report on Shell's impact on climate change...</td>\n",
              "      <td>The report makes assumptions about investment ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Shell plc</td>\n",
              "      <td>Hydrogen|Oil, gas &amp; coal</td>\n",
              "      <td>GB</td>\n",
              "      <td>NG</td>\n",
              "      <td></td>\n",
              "      <td>Oil, gas &amp; coal</td>\n",
              "      <td>08.11.2006</td>\n",
              "      <td>Shell response to Urgent Action statement by M...</td>\n",
              "      <td>Movement for the Survival of the Ogoni People ...</td>\n",
              "      <td>Clean, Healthy &amp; Sustainable Environment|Secur...</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Shell</td>\n",
              "      <td>https://www.business-humanrights.org/en/latest...</td>\n",
              "      <td>https://www.business-humanrights.org/en/compan...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Movement for the Survival of the Ogoni People ...</td>\n",
              "      <td>For many years, MOSOP has claimed that SPDC [S...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Companies           Company Sectors Company Headquarters Countries Regions  \\\n",
              "0  Shell plc  Hydrogen|Oil, gas & coal                   GB        IE           \n",
              "1  Shell plc  Hydrogen|Oil, gas & coal                   GB        NG           \n",
              "2  Shell plc  Hydrogen|Oil, gas & coal                   GB        NG           \n",
              "\n",
              "  Response Sectors    Backdate  \\\n",
              "0  Oil, gas & coal  02.06.2009   \n",
              "1  Oil, gas & coal  08.07.2009   \n",
              "2  Oil, gas & coal  08.11.2006   \n",
              "\n",
              "                                               Title  \\\n",
              "0               Shell response re Corrib gas protest   \n",
              "1            Shell response re climate change report   \n",
              "2  Shell response to Urgent Action statement by M...   \n",
              "\n",
              "                                        Responded To  \\\n",
              "0  Ireland: Archbishop Desmond Tutu raises concer...   \n",
              "1  NGO report on Shell's impact on climate change...   \n",
              "2  Movement for the Survival of the Ogoni People ...   \n",
              "\n",
              "                                                Tags Responded  \\\n",
              "0                                  Protests|Violence       Yes   \n",
              "1  Clean, Healthy & Sustainable Environment|Clima...       Yes   \n",
              "2  Clean, Healthy & Sustainable Environment|Secur...       Yes   \n",
              "\n",
              "                                             Authors  \\\n",
              "0  Terry Nolan, Managing Director, Shell E&P Ireland   \n",
              "1                                              Shell   \n",
              "2                                              Shell   \n",
              "\n",
              "                                                 URL  \\\n",
              "0  https://www.business-humanrights.org/en/latest...   \n",
              "1  https://www.business-humanrights.org/en/latest...   \n",
              "2  https://www.business-humanrights.org/en/latest...   \n",
              "\n",
              "                                Link to Company Page Story Response  \\\n",
              "0  https://www.business-humanrights.org/en/compan...                  \n",
              "1  https://www.business-humanrights.org/en/compan...                  \n",
              "2  https://www.business-humanrights.org/en/compan...                  \n",
              "\n",
              "                                          story_text  \\\n",
              "0  Ireland: Archbishop Desmond Tutu raises concer...   \n",
              "1  NGO report on Shell's impact on climate change...   \n",
              "2  Movement for the Survival of the Ogoni People ...   \n",
              "\n",
              "                                       response_text  \n",
              "0  We wish to state categorically that there was ...  \n",
              "1  The report makes assumptions about investment ...  \n",
              "2  For many years, MOSOP has claimed that SPDC [S...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === Config ===\n",
        "INPUT_PATH = \"20250925_1051_bhrrc_scraper_output.json\"  # or \"your_cases.jsonl\"\n",
        "INPUT_FORMAT = \"json\"           # \"csv\" or \"jsonl\"\n",
        "TEXT_COLUMNS = [\"story_text\", \"response_text\"]  # adapt if needed\n",
        "ID_COL = None  # if you have a unique ID column, put its name here\n",
        "DATE_COL = \"Backdate\"  # optional; we'll try to parse\n",
        "SAVE_PREFIX = \"unsup_themes\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, re, math\n",
        "from datetime import datetime\n",
        "\n",
        "def load_data(path, fmt=\"json\"):\n",
        "    if fmt == \"csv\":\n",
        "        df = pd.read_csv(path)\n",
        "    elif fmt == \"json\":\n",
        "        df = pd.read_json(path, lines=True)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported format\")\n",
        "    return df\n",
        "\n",
        "os.chdir(\"C:/Users/bscherrer/Documents/snf-project3\")\n",
        "\n",
        "df = load_data(INPUT_PATH, INPUT_FORMAT)\n",
        "print(\"Loaded rows:\", len(df))\n",
        "df.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d98686e7",
      "metadata": {},
      "source": [
        "## 2) Clean & prepare the corpus\n",
        "\n",
        "- Merge story & response into one `text` field per case (or keep both; we’ll default to a combined text).\n",
        "- Light normalization (whitespace, unicode). Keep punctuation and case (can help for names).\n",
        "\n",
        "We also add a short `doc_id` for traceability in later exports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b89365db",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After cleaning, rows: 199\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Ireland: Archbishop Desmond Tutu raises concerns over alleged assault on a protestor during protest against Shell's Corrib project - Business & Human Rights Resource Centre \\n\\n We wish to state categorically that there was no physical attack of any kind on Mr Corduff by anyone while he was present on our construction site. The security staff employed by the Corrib Gas Partners have been fully briefed on the ethical and behavioural standards they are expected to meet when engaging with community m\""
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import unicodedata\n",
        "\n",
        "def normalize_text(s):\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# Build a combined text field\n",
        "def combine_text(row, cols):\n",
        "    parts = [normalize_text(row.get(c, \"\")) for c in cols if c in row]\n",
        "    return \" \\n\\n \".join([p for p in parts if p])\n",
        "\n",
        "df[\"text\"] = df.apply(lambda r: combine_text(r, TEXT_COLUMNS), axis=1)\n",
        "\n",
        "# Create a doc_id for traceability\n",
        "if ID_COL and ID_COL in df.columns:\n",
        "    df[\"doc_id\"] = df[ID_COL].astype(str)\n",
        "else:\n",
        "    df[\"doc_id\"] = [f\"doc_{i:04d}\" for i in range(len(df))]\n",
        "\n",
        "# Parse Backdate if present (day.month.year common format)\n",
        "def parse_backdate(x):\n",
        "    if pd.isna(x):\n",
        "        return pd.NaT\n",
        "    x = str(x).strip()\n",
        "    for fmt in (\"%d.%m.%Y\", \"%Y-%m-%d\", \"%d/%m/%Y\", \"%m/%d/%Y\"):\n",
        "        try:\n",
        "            return datetime.strptime(x, fmt).date()\n",
        "        except:\n",
        "            pass\n",
        "    return pd.NaT\n",
        "\n",
        "if \"Backdate\" in df.columns:\n",
        "    df[\"date\"] = df[\"Backdate\"].apply(parse_backdate)\n",
        "else:\n",
        "    df[\"date\"] = pd.NaT\n",
        "\n",
        "# Filter empty texts\n",
        "df = df[df[\"text\"].str.len() > 0].reset_index(drop=True)\n",
        "print(\"After cleaning, rows:\", len(df))\n",
        "df[[\"doc_id\", \"Companies\", \"Company Sectors\", \"Countries\", \"text\", \"date\"]].head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64962c1f",
      "metadata": {},
      "source": [
        "## 3) Create embeddings\n",
        "\n",
        "Two options:\n",
        "1. **Sentence-Transformers** (default, local): e.g., `all-MiniLM-L6-v2` (fast) or `all-mpnet-base-v2` (higher quality).\n",
        "2. **OpenAI embeddings** (optional): requires API key; can be helpful if you already use their stack.\n",
        "\n",
        "We'll implement Sentence-Transformers first, with a toggle to switch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "acb3ad0a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\bscherrer\\AppData\\Local\\miniconda3\\envs\\scrape-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\bscherrer\\AppData\\Local\\miniconda3\\envs\\scrape-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bscherrer\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Batches: 100%|██████████| 7/7 [00:06<00:00,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding shape: (199, 384)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# === Embeddings (Sentence-Transformers by default) ===\n",
        "USE_OPENAI = False  # set True to use OpenAI embeddings instead\n",
        "SENTENCE_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # or 'all-mpnet-base-v2'\n",
        "\n",
        "embeddings = None\n",
        "\n",
        "if not USE_OPENAI:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    model = SentenceTransformer(SENTENCE_MODEL_NAME)\n",
        "    texts = df[\"text\"].tolist()\n",
        "    embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
        "else:\n",
        "    # OpenAI path (optional). Requires OPENAI_API_KEY in env.\n",
        "    # from openai import OpenAI\n",
        "    # client = OpenAI()\n",
        "    # def get_openai_embeds(batch):\n",
        "    #     resp = client.embeddings.create(\n",
        "    #         model=\"text-embedding-3-large\",\n",
        "    #         input=batch\n",
        "    #     )\n",
        "    #     return np.array([d.embedding for d in resp.data], dtype=\"float32\")\n",
        "    # # Batch and call API here...\n",
        "    raise NotImplementedError(\"Set USE_OPENAI=False or implement your OpenAI embedding code.\")\n",
        "    \n",
        "print(\"Embedding shape:\", None if embeddings is None else embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7872b0b9",
      "metadata": {},
      "source": [
        "## 4) Dimensionality reduction (UMAP)\n",
        "\n",
        "UMAP to:\n",
        "- 2D for plotting\n",
        "- ~15D (optional) for clustering stability\n",
        "\n",
        "HDBSCAN can work directly on the original embedding, but many practitioners cluster on a lower-d space for robustness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0730622",
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'module' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === UMAP dimensionality reduction ===\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# from umap import UMAP\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 2D for visualization\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m umap_2d = \u001b[43mUMAP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m X_2d = umap_2d.fit_transform(embeddings)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Optional: reduced space for clustering (often 10-15 dims)\u001b[39;00m\n",
            "\u001b[31mTypeError\u001b[39m: 'module' object is not callable"
          ]
        }
      ],
      "source": [
        "# === UMAP dimensionality reduction ===\n",
        "from umap import UMAP\n",
        "\n",
        "# 2D for visualization\n",
        "umap_2d = UMAP(n_components=2, random_state=42)\n",
        "X_2d = umap_2d.fit_transform(embeddings)\n",
        "\n",
        "# Optional: reduced space for clustering (often 10-15 dims)\n",
        "umap_hd = UMAP(n_components=15, random_state=42)\n",
        "X_hd = umap_hd.fit_transform(embeddings)\n",
        "\n",
        "df[\"umap_x\"] = X_2d[:,0]\n",
        "df[\"umap_y\"] = X_2d[:,1]\n",
        "\n",
        "print(\"UMAP shapes:\", X_2d.shape, X_hd.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f382bda2",
      "metadata": {},
      "source": [
        "## 5) Clustering\n",
        "\n",
        "- **HDBSCAN** (density-based, auto-detects number of clusters, handles noise)\n",
        "- Fallback: **k-means** (requires `n_clusters` choice)\n",
        "\n",
        "We save `cluster_id` per document. HDBSCAN may assign `-1` to noise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "241b3ae2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Clustering (HDBSCAN with fallback to k-means) ===\n",
        "cluster_labels = None\n",
        "algo = None\n",
        "\n",
        "try:\n",
        "    import hdbscan\n",
        "    clusterer = hdbscan.HDBSCAN(min_cluster_size=15, min_samples=None, metric='euclidean')\n",
        "    cluster_labels = clusterer.fit_predict(X_hd)  # or embeddings directly\n",
        "    algo = \"hdbscan\"\n",
        "    print(\"HDBSCAN clusters (incl. noise=-1):\", len(set(cluster_labels)))\n",
        "except Exception as e:\n",
        "    print(\"HDBSCAN unavailable or failed:\", e)\n",
        "    # Fallback to k-means with a heuristic k\n",
        "    from sklearn.cluster import KMeans\n",
        "    k = max(5, int(math.sqrt(len(df)) // 2))  # rough heuristic, tune as needed\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=\"auto\")\n",
        "    cluster_labels = kmeans.fit_predict(X_hd)\n",
        "    algo = \"kmeans\"\n",
        "    print(\"KMeans clusters:\", k)\n",
        "\n",
        "df[\"cluster_id\"] = cluster_labels\n",
        "df[\"is_noise\"] = (cluster_labels == -1) if algo == \"hdbscan\" else False\n",
        "df[\"cluster_id\"].value_counts().sort_index().head(20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bb4898c",
      "metadata": {},
      "source": [
        "## 6) Topic labels via keywords (c-TF-IDF)\n",
        "\n",
        "We compute per-cluster \"class-based TF-IDF\" keywords and short labels. This approximates BERTopic's labeling step.\n",
        "Later, you may refine labels with an LLM (optional cell below).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fb0b2e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === c-TF-IDF keyword extraction per cluster ===\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "def c_tf_idf(corpus, m, ngram_range=(1,2), min_df=2):\n",
        "    # corpus: list of concatenated documents per class\n",
        "    vectorizer = CountVectorizer(ngram_range=ngram_range, min_df=min_df, stop_words='english')\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    transformer = TfidfTransformer(norm=None, use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
        "    tf_idf = transformer.fit_transform(X)\n",
        "    # scale by class lengths (m = total documents)\n",
        "    tf_idf = tf_idf / m\n",
        "    return tf_idf, vectorizer\n",
        "\n",
        "def top_terms_per_class(tf_idf, vectorizer, topk=15):\n",
        "    terms = np.array(vectorizer.get_feature_names_out())\n",
        "    tops = []\n",
        "    for row in tf_idf:\n",
        "        idx = np.argsort(row.toarray()[0])[::-1][:topk]\n",
        "        tops.append(terms[idx].tolist())\n",
        "    return tops\n",
        "\n",
        "# Build class documents: concatenate texts per cluster\n",
        "clusters = sorted(df[\"cluster_id\"].dropna().unique().tolist())\n",
        "cluster_texts = []\n",
        "cluster_sizes = []\n",
        "\n",
        "for c in clusters:\n",
        "    texts_c = df.loc[df[\"cluster_id\"]==c, \"text\"].astype(str).tolist()\n",
        "    cluster_sizes.append(len(texts_c))\n",
        "    cluster_texts.append(\" \\n \".join(texts_c) if texts_c else \"\")\n",
        "\n",
        "tfidf_mat, vect = c_tf_idf(cluster_texts, m=len(df), ngram_range=(1,2), min_df=2)\n",
        "keywords_per_cluster = top_terms_per_class(tfidf_mat, vect, topk=15)\n",
        "\n",
        "cluster_labels_map = {}\n",
        "for c, kws in zip(clusters, keywords_per_cluster):\n",
        "    # Create a simple label from top 3 keywords\n",
        "    label = \", \".join(kws[:3]) if kws else \"misc\"\n",
        "    cluster_labels_map[c] = {\"label\": label, \"keywords\": kws}\n",
        "\n",
        "# Attach labels\n",
        "df[\"cluster_label\"] = df[\"cluster_id\"].map(lambda c: cluster_labels_map.get(c, {}).get(\"label\", \"misc\"))\n",
        "df[[\"doc_id\",\"cluster_id\",\"cluster_label\"]].head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08e8f6d0",
      "metadata": {},
      "source": [
        "## 7) Representative quotes / examples\n",
        "\n",
        "For each cluster, surface a few short snippets from the most central documents (centroid-based for k-means;\n",
        "for HDBSCAN, we pick documents with high membership probability if available, else nearest to cluster centroid in embedding space).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7fbe983",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Representative snippets per cluster ===\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "def get_representatives(emb_matrix, ids, n=5):\n",
        "    # pick n docs closest to centroid\n",
        "    if len(ids) == 0:\n",
        "        return []\n",
        "    idx = np.array(ids)\n",
        "    centroid = emb_matrix[idx].mean(axis=0, keepdims=True)\n",
        "    dists = pairwise_distances(emb_matrix[idx], centroid, metric=\"euclidean\").ravel()\n",
        "    order = np.argsort(dists)\n",
        "    return idx[order][:n].tolist()\n",
        "\n",
        "def shorten(text, n=240):\n",
        "    s = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return (s[:n] + \"…\") if len(s) > n else s\n",
        "\n",
        "cluster_summaries = []\n",
        "for c in clusters:\n",
        "    doc_idx = df.index[df[\"cluster_id\"]==c].tolist()\n",
        "    rep_idx = get_representatives(embeddings, doc_idx, n=5)\n",
        "    reps = []\n",
        "    for ridx in rep_idx:\n",
        "        row = df.iloc[ridx]\n",
        "        # use part of response_text if available, else combined text\n",
        "        base = row.get(\"response_text\", \"\") or row[\"text\"]\n",
        "        reps.append({\n",
        "            \"doc_id\": row[\"doc_id\"],\n",
        "            \"snippet\": shorten(base, 280)\n",
        "        })\n",
        "    cluster_summaries.append({\n",
        "        \"cluster_id\": int(c),\n",
        "        \"size\": int(len(doc_idx)),\n",
        "        \"label\": cluster_labels_map.get(c, {}).get(\"label\", \"misc\"),\n",
        "        \"keywords\": cluster_labels_map.get(c, {}).get(\"keywords\", []),\n",
        "        \"representatives\": reps\n",
        "    })\n",
        "\n",
        "# Preview a couple of clusters\n",
        "cluster_summaries[:2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c67afc11",
      "metadata": {},
      "source": [
        "## 8) Visualization & exports\n",
        "\n",
        "- 2D scatter plot of UMAP with cluster IDs\n",
        "- CSV export: `themes_assignments.csv` and `cluster_summaries.csv` including keywords and representative quotes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dc20f9a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Simple 2D scatter plot ===\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "plt.scatter(df[\"umap_x\"], df[\"umap_y\"], s=6)\n",
        "plt.title(\"UMAP projection (all docs)\")\n",
        "plt.xlabel(\"UMAP-1\")\n",
        "plt.ylabel(\"UMAP-2\")\n",
        "plt.show()\n",
        "\n",
        "# Export results\n",
        "assign_cols = [\"doc_id\",\"Companies\",\"Company Sectors\",\"Company Headquarters\",\"Countries\",\"date\",\"cluster_id\",\"cluster_label\",\"umap_x\",\"umap_y\"]\n",
        "assign_cols = [c for c in assign_cols if c in df.columns]\n",
        "assign_df = df[assign_cols].copy()\n",
        "assign_path = f\"{SAVE_PREFIX}_assignments.csv\"\n",
        "assign_df.to_csv(assign_path, index=False)\n",
        "\n",
        "# Summaries export\n",
        "import json\n",
        "summ_path = f\"{SAVE_PREFIX}_cluster_summaries.json\"\n",
        "with open(summ_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cluster_summaries, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "assign_path, summ_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2807be1",
      "metadata": {},
      "source": [
        "## 9) (Optional) LLM-assisted labeling\n",
        "\n",
        "If you want nicer human-readable labels, call an LLM with each cluster's top keywords and 2–3 example snippets.\n",
        "This cell is optional and requires an API key (commented by default).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a498155",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Optional: LLM-assisted label refinement (commented) ===\n",
        "# This cell sends top keywords + representative snippets to an LLM to produce cleaner labels.\n",
        "# Requires OPENAI_API_KEY in your environment.\n",
        "#\n",
        "# from openai import OpenAI\n",
        "# client = OpenAI()\n",
        "#\n",
        "# def refine_label(keywords, reps):\n",
        "#     prompt = f\"\"\"\n",
        "#     You are labeling a text cluster from corporate sustainability responses.\n",
        "#     Here are the top keywords: {', '.join(keywords)}\n",
        "#     Here are representative snippets:\n",
        "#     {chr(10).join('- ' + r['snippet'] for r in reps)}\n",
        "#     Suggest a short, human-readable label (max 6 words) and a one-sentence description.\n",
        "#     Respond as JSON with keys: label, description.\n",
        "#     \"\"\"\n",
        "#     resp = client.chat.completions.create(\n",
        "#         model=\"gpt-4o-mini\",\n",
        "#         messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "#         temperature=0.2\n",
        "#     )\n",
        "#     import json\n",
        "#     return json.loads(resp.choices[0].message.content)\n",
        "#\n",
        "# refined = {}\n",
        "# for cs in cluster_summaries:\n",
        "#     refined[cs[\"cluster_id\"]] = refine_label(cs[\"keywords\"], cs[\"representatives\"])\n",
        "#\n",
        "# refined\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "scrape-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
